# -*- coding: utf-8 -*-
"""5_Doc_similarity(hash)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ejotQaSe9pyR7BzqvuILmt4AreGoRZZU
"""

# Hashing Vectorizer를 사용하여 유사도 측정

import numpy as np
from sklearn.feature_extraction.text import HashingVectorizer

sent = ('휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요. 내일 은 반가운 비 소식 이 있습니다',
        '폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니다')

vocab_size = 20
hvectorizer = HashingVectorizer(n_features = vocab_size,
                               norm=None,
                               alternate_sign = False)
hash_matrix = hvectorizer.fit_transform(sent).toarray()
print(hash_matrix)

# L2 normalization
hvectorizer = HashingVectorizer(n_features = vocab_size, norm='l2', alternate_sign=False)
hash_matrix = hvectorizer.fit_transform(sent).toarray()
print(np.round(hash_matrix, 3))

# 자카드 유사도
sent_1 = set(sent[0].split())
sent_2 = set(sent[1].split())
print(sent_1)
print(sent_2)

hap_set = sent_1 | sent_2
gyo_set = sent_1 & sent_2 
print(hap_set, '\n')
print(gyo_set, '\n')

jaccard = len(gyo_set) / len(hap_set)
print(jaccard)

# 코사인 유사도
from sklearn.metrics.pairwise import cosine_similarity
d = cosine_similarity(hash_matrix[0:1], hash_matrix[1:2])
print(d)

# 유클리디안 유사도
from sklearn.metrics.pairwise import euclidean_distances

euclidean_distances(hash_matrix[0:1], hash_matrix[1:2])

def l1_normalize(v):
    return v / np.sum(v)

tfidf_norm_l1 = l1_normalize(hash_matrix)
d = euclidean_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])
print(d)