# -*- coding: utf-8 -*-
"""Kaggle Bags of Popcorn

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zzEDsGDZ9rOdNcHdWLvqEzWgREjh-RIq

# Kaggle Data
"""

! pip install -q kaggle
from google.colab import files
files.upload()

!pip install --upgrade --force-reinstall --no-deps kaggle

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets list

! kaggle competitions download -c 'word2vec-nlp-tutorial'
! mkdir word2vec_test
! unzip word2vec-nlp-tutorial -d word2vec_test

import zipfile

DATA_IN_PATH = '/content/word2vec_test/'

file_list = ['labeledTrainData.tsv.zip', 'unlabeledTrainData.tsv.zip', 'testData.tsv.zip']

for file in file_list:
  zipRef = zipfile.ZipFile(DATA_IN_PATH + file, 'r')
  zipRef.extractall(DATA_IN_PATH)
  zipRef.close()

import re
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
from nltk.corpus import stopwords
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import os
import nltk
nltk.download('stopwords')

train_data = pd.read_csv(DATA_IN_PATH+'labeledTrainData.tsv', 
                         header = 0, delimiter = '\t', quoting = 3)
train_data.head()

print('파일 크기 :')
for file in os.listdir(DATA_IN_PATH):
  if 'tsv' in file and 'zip' not in file:
    print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + 'MB')

DATA_IN_PATH = '/content/word2vec_test/'
train_data = pd.read_csv(DATA_IN_PATH+'labeledTrainData.tsv', header = 0, delimiter = '\t', quoting = 3)
print(train_data['review'][0])

def preprocessing(review, stops, remove_stopwords = False): 
    # 1. HTML 태그 제거
    review_text = BeautifulSoup(review, "html.parser").get_text()

    # 2. 영어가 아닌 특수문자들을 공백(" ")으로 바꾸기
    review_text = re.sub("[^a-zA-Z]", " ", review_text)

    # 3. 대문자들을 소문자로 바꾸고 공백단위로 텍스트들 나눠서 리스트로 만든다.
    words = review_text.lower().split()

    if remove_stopwords: 
        # 4. 불용어 제거       
        # 불용어가 아닌 단어들로 이루어진 새로운 리스트 생성
        words = [w for w in words if not w in stops]
        
        # 5. 단어 리스트를 공백을 넣어서 하나의 글로 합친다.
        clean_review = ' '.join(words)

    else: # 불용어를 제거하지 않을 때
        clean_review = ' '.join(words)

    return clean_review

stops = set(stopwords.words("english"))
clean_train_reviews = []
for review in train_data['review']:
    r = preprocessing(review, stops, remove_stopwords = True)
    clean_train_reviews.append(r)

# 전처리가 완료된 리뷰 데이터를 데이터프레임으로 구성한다. (학습할 데이터)
clean_train_df = pd.DataFrame({'review': clean_train_reviews, 
                               'sentiment': train_data['sentiment']})

# 리뷰 데이터를 워드 인덱스로 표현한다.
tokenizer = Tokenizer()
tokenizer.fit_on_texts(clean_train_reviews)
text_sequences = tokenizer.texts_to_sequences(clean_train_reviews)
print(text_sequences[0])
len(text_sequences[0])

# 리뷰 데이터의 길이를 통일시킨다.
# 길이는 중간값인 174로 하고, 리뷰 데이터의 워드 인덱스 길이가 174보다 작으면 
# 뒷 부분을 0으로 패딩하고, 174보다 크면 뒷 부분을 버린다.
MAX_SEQUENCE_LENGTH = 174 

train_inputs = pad_sequences(text_sequences, 
                             maxlen=MAX_SEQUENCE_LENGTH,
                             padding='post',
                             truncating='post')
print('Shape of train data: ', train_inputs.shape)

print(train_inputs[0])
print("길이 = ", len(train_inputs[0]))

# 리뷰 문서의 라벨 (1 or 0)을 가져온다
train_labels = np.array(train_data['sentiment'])
print('Shape of label tensor:', train_labels.shape)
train_labels[0]

TRAIN_INPUT_DATA = 'train_input.npy'
TRAIN_LABEL_DATA = 'train_label.npy'
TRAIN_CLEAN_DATA = 'train_clean.csv'

# 전처리가 완료된 학습 데이터를 파일에 저장해 둔다
TRAIN_INPUT_DATA = '4-1.train_input.npy'
TRAIN_LABEL_DATA = '4-1.train_label.npy'
TRAIN_CLEAN_DATA = '4-1.train_clean.csv'
DATA_IN_PATH = '/content/word2vec_test/'

# 전처리 된 데이터를 넘파이 형태로 저장
np.save(open(DATA_IN_PATH + TRAIN_INPUT_DATA, 'wb'), train_inputs)
np.save(open(DATA_IN_PATH + TRAIN_LABEL_DATA, 'wb'), train_labels)

# 정제된 텍스트를 csv 형태로 저장
clean_train_df.to_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA, index = False)